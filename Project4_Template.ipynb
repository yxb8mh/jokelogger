{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nhhQxcu1_KUI"
      },
      "source": [
        "# DS 3025 Final Project: Neural Network From Scratch\n",
        "\n",
        "**General Instructions:** For this project, you will be creating the code for a simple Neural Network from scratch. The beginning portion of the project will match closely with what we have completed together during Lab. You will first create some helper functions then the larger building blocks of the network. After creating the network, you will be asked to tweak different pieces and run experiments to demonstrate your understanding of the underlying code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1_RfeZw9tTTJ"
      },
      "source": [
        "**Type the Honor Pledge Here:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icBs_uoLvYo6"
      },
      "source": [
        "### Tips for Success on this Project:\n",
        "- use `print` statements to debug your code\n",
        "- code small helper functions first and check if correct by running practice tests\n",
        "- check if matrices are the **correct dimension for matrix multiplication** by printing the matrix or printing the dimension of the matrix (may need to transpose by using `matrix.T` in some cases)\n",
        "- make sure you understand the math behind feeding forward and backpropogating before implementing the `feed_forward` and `gradient` functions\n",
        "- The internet is your friend! Look up unknown python syntax and error messages  \n",
        "- Other potentially useful functions: `np.dot`, `np.random.normal`, `np.exp`, `pd.DataFrame`\n",
        "- Ensure your cost function is correct (it might be different from lab). Ensure you are doing MSE and dividing by the length of the each dataset at the proper time"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gh1Fp186ko-m"
      },
      "source": [
        "### Additional Help with Python Coding:\n",
        "Reach out to Ali Rivera (wat6sv@virginia.edu)! She is very nice and hosts python office hours from 12-1 on Wednesdays in Room 300 (she has snacks!)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QyqAUAZ7tI55"
      },
      "source": [
        "### Grading\n",
        "\n",
        "* Only build off of the code and functions given to you in lab.\n",
        "* Do not import any additional modules/functions aside from the ones given at the beginning of the project.\n",
        "* Do not use any additional data aside from what we define to be X and Y in the beginning for your training. You will receive a 0 if you add columns or create new testing data in any way.\n",
        "* Points will be deducted if your code is not reproducible. I.e. your output should look the same as when we run your uploaded notebook in google colab (remember to keep your random seed as we have set it to 42). Before you submit - restart your kernel in Google Colab and run all of your code from bottom to top - if nothing changes, you are good to go.\n",
        "* Points will be deducted for redundant code, i.e. repeating function definitions in every code block. Note: Re-defining functions to add new parts is not considered redundant code.\n",
        "* Your code has to work, but we are focused more on the summmary and explainations we ask you about the code. We want to know that you understand the ideas behind the code. When in doubt, document your through process and resources used. If you run into issues, detail them in your submission so we can give you partial credit.\n",
        "* We will be testing your final model parameters on a synthetic dataset we have generated from the one you are given. Your model's accuracy on this new test set will be recorded and ranked across students from both sections. Your overall grade will be affected by where you fall in this rank. In order to perform well on this, you will need to ensure that your model's test accuracy is as high as possible without overfitting or underfitting your model.\n",
        "* A ruberic with more specific details on grade brackets will follow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w35-9uGsr_7L"
      },
      "source": [
        "# Project Overview\n",
        "\n",
        "## Part 1:\n",
        "Your goal for Part 1 will be to carefully copy over functions from Lab 11 & 12 and ensure all your functions are in working order for this particular data set and problem. The steps for this part will include:\n",
        "\n",
        "- Load your data\n",
        "- Define your helper funtions\n",
        "- Create your \"Major\" functions (1 Hidden Layer Network)\n",
        "- Test your network with the functions you created given a defined set of parameters\n",
        "\n",
        "## Part 2:\n",
        "Your goal for Part 2 will be to build off of the functions built in Lab (Part 1). This will include:\n",
        "\n",
        "- Adding a new activaiton function (Hyperbolic Tangent)\n",
        "- Creating a new neural network (2 Hidden Layer Network)\n",
        "- Comparing the performance of the 1 vs 2 Hidden Layer Network.\n",
        "\n",
        "## Part 3: Experimentation/Parameter Tuning (with 1-layer model)\n",
        "\n",
        "For this section, you will be conduct 4 different experiments that test:\n",
        "- A Baseline model\n",
        "- Varying the Activation Functions\n",
        "- Varying the Learning Rate\n",
        "- Varying the \\# of Hidden Layer Nodes\n",
        "\n",
        "From each of these experiments, you will produce a graph, table, and summary.\n",
        "\n",
        "## Part 4: Further Experimentation/Parameter Tuning\n",
        "\n",
        "In this part you will experiment further to find the values of the parameters (activation function, epochs, learning rate, hidden nodes) that best train your model to produce the highest test accuracy ON OUR SYNTHETIC DATASET. This means that you need to find the parameters that not only produce the best test accuracy but also don't overfit the training data.\n",
        "\n",
        "## Part 5: Comprehension Questions\n",
        "\n",
        "You will be asked 3 comprehension questions, 2 will require code for you to answer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYqkiGWZ_WZu"
      },
      "source": [
        "#### Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8I7vPlzF_IUf"
      },
      "outputs": [],
      "source": [
        "#### DO NOT ADJUST ####\n",
        "# The only libraries we will be using\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import normalize\n",
        "from sklearn.datasets import load_iris\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f3KvJbpZQMhV"
      },
      "outputs": [],
      "source": [
        "#### DO NOT ADJUST ####\n",
        "# Define seed\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "92trm9wwPaYi"
      },
      "source": [
        "### Load Data\n",
        "\n",
        "For this project, we're using a new dataset to classify wine type between red and white. We have the following measurements of each wine \"subject\" (`fixed acidity`, `volatile acidity`, `residual sugar`, `chlorides`, `alcohol`, `quality`). Our goal is to predict the type of wine (red or white). This means we are dealing with a binary classification problem. Our model will output a probability that it a red wine (because red is coded as 1).\n",
        "\n",
        "Below is the code to load in the data sets to X and Y. Do **not** change the code below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9lfjTigmNRg"
      },
      "outputs": [],
      "source": [
        "#### DO NOT ADJUST ####\n",
        "\n",
        "## Load in the Wine Quality Data\n",
        "red = pd.read_csv('https://raw.githubusercontent.com/shrikant-temburwar/Wine-Quality-Dataset/refs/heads/master/winequality-red.csv', sep =';')\n",
        "white = pd.read_csv('https://raw.githubusercontent.com/shrikant-temburwar/Wine-Quality-Dataset/refs/heads/master/winequality-white.csv', sep =';')\n",
        "\n",
        "# Random sample to decrease the sample size of red and white to 200 per class\n",
        "red = red.sample(200, random_state=seed)\n",
        "red.drop(['density', 'pH', 'citric acid', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates'], axis = 1, inplace = True)\n",
        "white = white.sample(200, random_state=seed)\n",
        "white.drop(['density', 'pH', 'citric acid', 'free sulfur dioxide', 'total sulfur dioxide', 'sulphates'], axis = 1, inplace = True)\n",
        "print(white.columns)\n",
        "\n",
        "# add columns for classification\n",
        "red['type'] = 0\n",
        "white['type'] = 1\n",
        "\n",
        "# combine the data frames\n",
        "X = pd.concat([red, white], axis=0)\n",
        "\n",
        "# Now seperate X and Y\n",
        "Y = X['type']\n",
        "X = X.drop('type', axis=1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cwFE2ctZe4dp"
      },
      "source": [
        "# Part 1\n",
        "Your goal for Part 1 will be to ensure all of your functions are in working order and lay the groundwork for what ranges your experimentation in Part 2 should fall in\n",
        "\n",
        "- Convert your data to numpy and train/test split\n",
        "- Define your helper funtions\n",
        "- Create your \"Major\" functions (1 Hidden Layer Network)\n",
        "- Test your network with the functions you created given a defined set of parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jkkBIZchPaYj"
      },
      "source": [
        "### Q1 (a) . Split the data into test (`X_test`, `Y_test`) and train (`X_train`, `Y_train`) datasets. Standardize the training and testing data. ENSURE THAT YOUR VARIABLES ARE NAMED CORRECTLY"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1RX4q2bjPaYj"
      },
      "outputs": [],
      "source": [
        "# Convert X and Y from pandas to numpy arrays\n",
        "\n",
        "\n",
        "# Train/test Split\n",
        "\n",
        "\n",
        "# Standardize X_train and X_test\n",
        "\n",
        "\n",
        "\n",
        "# Reshape Y_train and Y_test to nx1 matrices\n",
        "\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "# Check X Train and Test Shapes\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "print(f'Y_train shape: {Y_train.shape}') #notice how there's a 1 in the column space after reshaping\n",
        "print(f'Y_test shape: {Y_test.shape}') #notice how there's a 1 in the column space after reshaping"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GlgV6J8mRxub"
      },
      "source": [
        "####**Q1 (b): What is going to be your number of input and output nodes - **Why? (be careful with your reasning for your output nodes)****\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Q9QYuGkwp-p"
      },
      "source": [
        "### Create the following Helper Functions:\n",
        "\n",
        "Pay close attention to the formulas and directions. Do not change the names of these functions.\n",
        "\n",
        "1. `sigmoid(x)`\n",
        "1. `sigmoid_derivative(x)`\n",
        "1. `relu(x)`\n",
        "1. `relu_derivative(x)`\n",
        "1. `ActivationFxn(x, activation_type)`\n",
        "1. `ActivationFxnDerivative(x, activation_type)`\n",
        "1. `cost(y, y_hat)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "59FTFy1r_a9z"
      },
      "source": [
        "#### Q3. Create a function called `sigmoid(x)`.\n",
        "\n",
        "This will be one of your **activation functions** that takes some vector `x` and returns the sigmoid of the vector. The `sigmoid` function can be written as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma(x) = \\frac{1}{1 + e^{-x}}\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNxlN7Gy_SKP"
      },
      "outputs": [],
      "source": [
        "def sigmoid(x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KMXfQ2JI_uar"
      },
      "source": [
        "#### Q4. Create a function called `sigmoid_derivative(x)`.\n",
        "\n",
        "This function will calculate the derivative of the activation function for backpropogation in the network at some point, `x`. The function should return the derivative of the sigmoid function at that point. The derivative of the Sigmoid function is written as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma^\\prime (x) = \\sigma(x) * (1 - \\sigma(x))\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Phy9fg0C_tuv"
      },
      "outputs": [],
      "source": [
        "def sigmoid_derivative(x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clOgLyP242bk"
      },
      "source": [
        "#### Q5. Create a function called `relu(x)`.\n",
        "\n",
        "This function will calculate and return the `ReLU` of some input value `x`. The ReLU function can be written as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma(x) = \\begin{cases} x, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases} = max(0, x)\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g9C18s_05Mcp"
      },
      "outputs": [],
      "source": [
        "def relu(x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8ZXNdkx5RGR"
      },
      "source": [
        "#### Q6. Create a function called `relu_derivative(x)`.\n",
        "\n",
        "This will calculate the derivative of the `ReLU` function for the given input. The derivative of ReLU can be written as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma^{\\prime}(x) = \\begin{cases} 1, & x > 0 \\\\ 0, & x \\leq 0 \\end{cases}\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dj3PPsf25m8t"
      },
      "outputs": [],
      "source": [
        "def relu_derivative(x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uJbZffL95rt6"
      },
      "source": [
        "#### Q7. Create a function called `ActivationFxn(x, activation_type)`.\n",
        "\n",
        "This will be a function that combines the two activation functions that you defined above. Given the activation type determined by `activation_type` (either `sigmoid` or `relu` for now), the function should calculate the activation function of the value `x`. Hint: You can use `if` statements to check for what type of `acitvation_type` the user wants. Make sure to have a condition for when the provided `activation_type` is not valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TJEKzdgd50a8"
      },
      "outputs": [],
      "source": [
        "# Combine your activation functions into one function\n",
        "def ActivationFxn(x, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d757QNT51-L"
      },
      "source": [
        "#### Q8. Create a function called `ActivationFxnDerivative(x, activation_type)`.\n",
        "\n",
        "This will be a function that combines the two activation functions derivatives that you defined above. Given the activation type determined by `activation_type` (either `sigmoid` or `relu` for now), the function should calculate the activation function derivative of the value `x`. Hint: You can use `if` statements to check for what type of `acitvation_type` the user wants. Make sure to have a condition for when the provided `activation_type` is not valid."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FCZIe31E52PW"
      },
      "outputs": [],
      "source": [
        "# Define the derivative of the activation function\n",
        "def ActivationFxnDerivative(x, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6GZ7BP3P6KQm"
      },
      "source": [
        "#### Q9. Create a function called `cost(y, y_hat)`.\n",
        "\n",
        "For this function, `y` is the true wine type and `y_hat` is the predicted probability of a red wine. The equation should use is:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\text{cost}(x) = \\sum (y - y\\_\\text{hat})^2\n",
        "\\end{gather}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nmz_dKx_9pWL"
      },
      "outputs": [],
      "source": [
        "def cost(y, y_hat):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Da3qF9mcAL8W"
      },
      "source": [
        "### Major Functions\n",
        "\n",
        "Congratulations! You've finished creating the helper functions that we will need throughout building this neural network. Next, we will work on the larger functions that we will need. The list of the functions you will develop is shown below along with each of the inputs they will take.\n",
        "\n",
        "1. `forward_propagation(x, activation_type)`\n",
        "1. `output(X, activaiton_type)`\n",
        "1. `gradient(x, y, a1, a2, activation_type)`\n",
        "1. `train(inputs, outputs, learning_rate, epochs, activation_type)`\n",
        "1. `test(inputs, outputs, activation_type)`\n",
        "1. `GenerateAllWeights(input_nodes, hidden_nodes, output_nodes, seed)`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMYIW1LKvwYi"
      },
      "source": [
        "#### Q10. Create a function called `forward_propogation(x, activation_type)`.\n",
        "\n",
        "The function should take an input vector `x` and apply a linear transformation and the specified activation function for the input layer to the hidden layer. The function should return the output values from the input to hidden neurons and hidden to output neuron(s). For example, the output of the 1st pass (input to hidden) could be called `z1` for before activation and `a1` = $(a_{11}, a_{12}, a_{13})$ for after activation. Likewise, the 2nd pass (output from the hidden to output) could be called `z2` before activation and `a2` = $(a_{21})$ after."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "prcj5_dTAOIn"
      },
      "outputs": [],
      "source": [
        "def forward_propagation(x, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlMU0xwFuNSo"
      },
      "source": [
        "### Q11. Create a function called `output(X, activation_type)`.\n",
        "\n",
        "The function should take in a data matrix, `X`. It should loop over each row of the matrix, `X`. For each row or sample, calculate the forward propogation using the function you defined above and the activation function determined by `activation_type`. Catch all elements that the `forward_propogation` function returns. Save the final output from the output layer, `a2`, into a list. The `output()` function should ultimately return this list of outputs. For a sanity check, you can make sure that the output list is the same length as the number of rows in `X`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vS7MDUpkuPpa"
      },
      "outputs": [],
      "source": [
        "def output(X, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a755pbdWvyx-"
      },
      "source": [
        "#### Q12. Create a function to calculate the gradient of the network. This function should be called `gradient(x,y,a1,a2,z1, z2, activation_type)`.\n",
        "\n",
        "It should take inputs `x`, `y`, `a1`, `a2`, `z1`, and `z2`. `x` is the input data to the layer. This is a given row of our data matrix. `y` is the true value of the wine class. `z1` is the value calculated from the pass from the input layer to the output layer before activation. `a1` is the result of `z1` being passed into the activation function. `z2` is the value from the hidden layer to the output layer before activation. `a2` is the reuslt from passing `z2` into the activation function and represents what the prediction is. Overall, this function should calculate what the gradient is for each weight and bias. For equations, refer back to Lab 11 & 12. The function should return the direction of the gradient for each weight matrix, labeled `dW1`, `dW2`, and biases labeled `db1`, `db2`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "chidIkphAo7s"
      },
      "outputs": [],
      "source": [
        "def gradient(x, y, a1, a2, z1, z2, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2y3vK11vv1M4"
      },
      "source": [
        "#### Q13. Create a function that trains the network. It should be called `train(x_train, y_train, x_test, y_test, learning_rate, epochs, activation_type)`.\n",
        "\n",
        "The function should apply each of the helper functions you defined before in the correct order. The function should return the cost values for both the training and testing data in lists."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LwI0YMSecIkW"
      },
      "outputs": [],
      "source": [
        "def train(x_train, y_train, x_test, y_test, learning_rate, epochs, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IbroYr23v3WQ"
      },
      "source": [
        "#### Q14. Create a function to test your network. It should be called `test(inputs, outputs, activation_type)`.\n",
        "\n",
        "You can run the function after training to see if neural network works well on other data points that it didn't see during training. For inputs, it should take in test `inputs` (x values) and `outputs` (y values) and print the `percent_correct`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5nrP_WdgAvvN"
      },
      "outputs": [],
      "source": [
        "def test(inputs, outputs, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gD8DMZkWwB_w"
      },
      "source": [
        "#### Q15. Create a function that initializes the values of the weights `W1`, `W2` and biases `b1`, `b2`. It should be called `GenerateAllWeights(input_nodes, hidden_nodes, output_nodes, seed)`\n",
        "\n",
        "**NOTE**: make sure to re-run `GenerateAllWeights` each time you train the network (otherwise `train` function will have edited the matices and you want them to start random)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zHCL-NcPF3Hc"
      },
      "outputs": [],
      "source": [
        "def GenerateAllWeights(input_nodes, hidden_nodes, output_nodes, seed):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrWgoHPvwC_t"
      },
      "source": [
        "### Q16. Train the Network, putting all of your functions together\n",
        "\n",
        "Next, we will put all of our functions together to actually train the neural network. Start by defining the number of epochs and the learning rate. For this part, use values of `hidden_nodes=3`, `epochs=500`, `learning_rate=0.1`, and `activation_type = 'sigmoid'`. Once the parameters are set, go ahead and define your weights, train the network, and plot the MSE for the training and testing set respecively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9NOkYWanA7Da"
      },
      "outputs": [],
      "source": [
        "# Define the number of input, hidden, and output nodes (note: nodes not layers)\n",
        "input_nodes =\n",
        "hidden_nodes =\n",
        "output_nodes ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jFl3qrt3A8lo"
      },
      "outputs": [],
      "source": [
        "# Define your parameters\n",
        "\n",
        "\n",
        "# Define your weights and biases\n",
        "\n",
        "\n",
        "# Train your network\n",
        "\n",
        "\n",
        "# Adjust costs where necessary (be mindful of what your cost function is calculating)\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "plt.plot(np.arange(epochs), train_cost, label = 'Train Cost', color = 'dodgerblue')\n",
        "plt.plot(np.arange(epochs), test_cost, label = 'Test Cost', color = 'firebrick')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(f'MSE For Train vs Test Data ({epochs} Epochs)')\n",
        "plt.ylim([0, np.max([np.max(test_cost), np.max(train_cost)])])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_DllX70pwFSY"
      },
      "source": [
        "#### Q17. Test the Network after Training\n",
        "\n",
        "Now that you've trained the model, we can test it. Use the `test` function on the `x_test` and `y_test` values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xEBQLinhA-Fd"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GDq9wy4gfm5i"
      },
      "source": [
        "# Part 2\n",
        "Your goal for Part 2 will be to build off of the functions built in Lab 11 & 12 (Part 1).\n",
        "\n",
        "- Re-define your activation function**s** by adding the Hyperbolic Tangent (`tanh`)\n",
        "- Create fucntions for forward propogation, output, gradient, and train that allow for a second hidden layer\n",
        "- Compare your 2-hidden-layer model to your 1-hidden-layer model (using the same parameters)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bqeAVmIKy-Yt"
      },
      "source": [
        "### Creating a New Activation Function"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ziNYXfhPuziV"
      },
      "source": [
        "### Q1. There are other activation functions than just Sigmoid and ReLU. One we talked about in class is the Hyperbolic Tangent function `tanh()`. Create a new function called `tanh()`.\n",
        "\n",
        "Note: for this question, and this question only, you are restricted to only using the `np.exp()` function from Numpy. Do **not** directly use `np.tanh()` for this question.\n",
        "\n",
        "Hint: The equation for the hyperbolic tangent is:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f0sCEVBboNyY"
      },
      "outputs": [],
      "source": [
        "def tanh(x):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhHzrtt0wHXE"
      },
      "source": [
        "#### Q2. Plot your activation function for $X \\in [-10, 10]$ with 100 data points. This is a way to confirm that your function behaves how the hyperbolic tanh does."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S2giwpAHvzLL"
      },
      "outputs": [],
      "source": [
        "# create X vector as described above (refer to Lab 12 for help) to define the first parameter of the plot\n",
        "\n",
        "\n",
        "# use the function you just created for tanh to define the second parameter of the plot\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "plt.plot(X, tanh_vals)\n",
        "plt.title(r'Hyperbolic Tangent')\n",
        "plt.xlabel(r'$x$')\n",
        "plt.ylabel(r'$\\sigma(x)$')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyNbsUNtwS7R"
      },
      "source": [
        "#### Q3. Using your `tanh()` function, add it to the `ActivationFxn()` function. The `activation_type` parameter should be `tanh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f57LJ0cgv1lc"
      },
      "outputs": [],
      "source": [
        "# Add tanh as an option to your ActivationFxn (after relu and sigmoid)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XoPg6_5w2zW"
      },
      "source": [
        "#### Q4. Define a `tanh_derivative()` function.\n",
        "\n",
        "The derivative of the hyperbolic tangent is the hyperbolic secant. The function can be defined as:\n",
        "\n",
        "\\begin{gather}\n",
        "  \\sigma^{\\prime} (x) = \\frac{2}{e^{x} + e^{-x}}\n",
        "\\end{gather}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhYcI4uewiak"
      },
      "outputs": [],
      "source": [
        "def tanh_derivative(x):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hKFqOL_RxofX"
      },
      "outputs": [],
      "source": [
        "# Call the tanh function on X (X is already defined above) to define the parameter needed for the first plot\n",
        "\n",
        "# Call the tanh derivative function on X to define the parameter needed for the second plot\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "plt.plot(X, tanh_values, label = 'Hyperbolic Tangent')\n",
        "plt.plot(X, tanh_derivative_values, label = 'Derivative of Hyperbolic Tangent')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EhLLd0J3xZjY"
      },
      "source": [
        "#### Q5. Add the `tanh_derivative()` to the `ActivaitonFxnDerivative()` function. The `activation_type` value should be called `tanh`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oq0fVi4KxY00"
      },
      "outputs": [],
      "source": [
        "# Define the derivative of the activation function\n",
        "def ActivationFxnDerivative(x, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sIz2s9TbhA1-"
      },
      "source": [
        "### Creating a 2 Hidden Layer Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmJuUysghA9A"
      },
      "source": [
        "#### Q6. Edit your network to include an additional hidden layer. This includes adding more weights and biases. Add a suffix `2` to the end of each function to indicate that it is the function for your 2 hidden layer network. The functions for you to change are:\n",
        "\n",
        "* `GenerateAllWeights()` $\\rightarrow$ `GenerateAllWeights_2()`\n",
        "* `forward_propogration()` $\\rightarrow$ `forward_propagation_2()`\n",
        "* `output()` $\\rightarrow$ `output_2()`\n",
        "* `gradient()` $\\rightarrow$ `gradient_2()`\n",
        "* `train()` $\\rightarrow$ `train_2()`\n",
        "* `test()` $\\rightarrow$ `test_2()`\n",
        "\n",
        "NOTE: Be careful to use the new functions with a suffix of `2` when you want to work with the 2 hidden layer network. If you accidentatly use the original functions with no suffix, you will run into errors. If you have errors that you run into, make sure to double check that you're using the new functions in all of the necessary positions.\n",
        "\n",
        "Start with both hidden layers containing `3` nodes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fUWwbw5rgnkG"
      },
      "outputs": [],
      "source": [
        "# define network structure\n",
        "input_nodes =\n",
        "first_hidden_nodes =\n",
        "second_hidden_nodes =\n",
        "output_nodes ="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GbbyfpXngnmD"
      },
      "outputs": [],
      "source": [
        "def GenerateAllWeights_2(input_nodes, first_hidden_nodes, second_hidden_nodes, output_nodes, seed):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TyrmX_wYgnqf"
      },
      "outputs": [],
      "source": [
        "def forward_propagation_2(x, activation_type):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ckCcPh09gnsU"
      },
      "outputs": [],
      "source": [
        "def output_2(X, activation_type):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rxWj5Ojheyo"
      },
      "outputs": [],
      "source": [
        "def gradient_2(x, y, a1, a2, a3, z1, z2, z3, activation_type):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I8TFbnlUhe0q"
      },
      "outputs": [],
      "source": [
        "def train_2(x_train, y_train, x_test, y_test, learning_rate, epochs, activation_type):\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6-apaga6he2w"
      },
      "outputs": [],
      "source": [
        "def test_2(inputs, outputs, activation_type):\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wn13iCnIhsDU"
      },
      "source": [
        "####Q7. Now that you've defined your functions, train your new 2 layer network. Plot the training and testing errors. Use parameters of `learning_rate=0.1`, `epochs=500`, and `activation_type = 'sigmoid'`.\n",
        "\n",
        "Make sure to name your outputs from train_2 `train_cost_2` and `test_cost_2` so as not to overwrite your train_cost and test_cost values from your 1-layer model! (If you do overwrite these values, make sure to go back and re-run your 1 hidden layer model training code chunk)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zj9IfwcvhtqM"
      },
      "outputs": [],
      "source": [
        "# Define the parameters for learning rate and epochs\n",
        "\n",
        "\n",
        "# Generate the weights and biases\n",
        "\n",
        "\n",
        "# Train the network, gathering the training and test error (remember what your cost function is currently calculating in your train loop)\n",
        "\n",
        "\n",
        "# Calculate MSE from you returned costs (remember what your cost function is currently calculating in your train loop)\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "plt.plot(np.arange(epochs), train_cost_2, label = 'Train Cost', color = 'dodgerblue')\n",
        "plt.plot(np.arange(epochs), test_cost_2, label = 'Test Cost', color = 'firebrick')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(f'MSE For Train vs Test Data ({epochs} Epochs)')\n",
        "plt.ylim([0, np.max([np.max(test_cost_2), np.max(train_cost_2)])])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "REKRMdkKh7xT"
      },
      "source": [
        "#### Q8. Calculate your Test Accuracy using your `test_2()` function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8xcXZZpdiB7U"
      },
      "outputs": [],
      "source": [
        "# Calculate the accuracy on the test set using your test_2 function\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjRUxDxMiXRt"
      },
      "source": [
        "#### Plot the train and test errors for both of your models: 1 hidden layer vs 2 hidden layers. Just run this code chunk below. There is no need for you to adjust the code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGZNVj6AiFKm"
      },
      "outputs": [],
      "source": [
        "# Define the plot\n",
        "fig, ax = plt.subplots(1, 2, figsize = (10, 4))\n",
        "\n",
        "# Plotting 1 layer train plot\n",
        "ax[0].plot(np.arange(epochs), train_cost, label = '1 Hidden Layer', color = 'dodgerblue')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Cost')\n",
        "ax[0].set_title(rf'Cost For Train ({epochs} Epochs, $\\eta$ = {learning_rate})')\n",
        "\n",
        "# Plotting 1 layer test plot\n",
        "ax[1].plot(np.arange(epochs), test_cost, label = '1 Hidden Layer', color = 'dodgerblue')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[1].set_ylabel('Cost')\n",
        "ax[1].set_title(rf'Cost For Test ({epochs} Epochs, $\\eta$ = {learning_rate})')\n",
        "\n",
        "# Plotting 2 layer train plot\n",
        "ax[0].plot(np.arange(epochs), train_cost_2, label = '2 Hidden Layers', color = 'firebrick')\n",
        "\n",
        "# Plotting 2 layer test plot\n",
        "ax[1].plot(np.arange(epochs), test_cost_2, label = '2 Hidden Layers', color = 'firebrick')\n",
        "\n",
        "# Final line to add legends and show the plot\n",
        "ax[0].legend()\n",
        "ax[1].legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpBkpMgxRnfN"
      },
      "source": [
        "#### **Q9: Compare your 1 hidden layer and 2 hidden layer models from the plots above in words below.**\n",
        "\n",
        "- Compare the accuracy (you calculated and printed these earlier) of both models. Remember this is the output from the `test()` and `test_2()` functions.\n",
        "- Next, compare the train and test MSE (`cost / n`) for both models.\n",
        "- Which model produced the best test accuracy?\n",
        "- Which model performed better? Justify your answer with the results shown in your code above and your generated plots.\n",
        "- Did your 2-layer model overfit? Or did it underfit? Does adding more hidden layers guarentee better performance?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Thk46ZkBgpLp"
      },
      "source": [
        "## Part 3: Experimentation/Parameter Tuning (with 1-hidden-layer model)\n",
        "Below you will run a series of experiments on your 1 Hidden Layer network. We have a series of values, or `hyperparameters`, that we can manipulate to change how the network behaves. Those `hyperparameters` for our case are: `Activation Function`, `Learning Rate`, `# of Hidden Nodes`. For each experiment, we will change only one variable at a time and keep the others constant. This allows us to isolate the effects of the `hyperparameters`.\n",
        "\n",
        "We will be walking you through 4 different experiments that test:\n",
        "- Our Baseline model\n",
        "- Activation Functions: `'sigmoid'`, `'relu'`, and `'tanh'`\n",
        "- Learning Rates: `0.5`, `0.1`, `0.01`, and `0.001`\n",
        "- \\# of Hidden Nodes: `3`, `5`, `7`, and `9`\n",
        "\n",
        "while keeping the other parameters constant.\n",
        "\n",
        "For each experiment (except for the baseline test), produce\n",
        "\n",
        "a) A line graph comparing each parameter's train and test costs over each epoch.\n",
        "\n",
        "b) Rows to be added to a table that lists the parameter you're testing, the final train cost, the final test cost, and the time it took to train. These will be combined to create a final table at the end of this section.\n",
        "\n",
        "c) A summary of which parameter performed the best and why for this given experiemnt.\n",
        "\n",
        "From here you should have a solid baseline as to which parameter values you want to test further in Part 4 to find the best parameters for your final model you submit for grading.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2rfXY6aeFPOg"
      },
      "source": [
        "#### More instructions on the table(s):\n",
        "\n",
        "You will create one table that contains the results for all 4 experiments. This will allow you to compare your experimental results to the baseline shown in the first row. For each experiment, you will track your parameters, what the train and test cost are, as well as the time it took to train. It will look like the markdown table below, but you will fill in the question marks and the other rows.\n",
        "\n",
        "| Activation Function | Learning Rate | Epochs | Hidden Nodes | Train Cost | Test Cost| Time to Train |\n",
        "| --------- |  --------- |  --------- |  --------- |  --------- |  ---------|  --------- |\n",
        "| Sigmoid | 0.1  | 500 | 3 | ?  | ? | ? |\n",
        "|  | | | |  |  | |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6crNpbpAYsZN"
      },
      "outputs": [],
      "source": [
        "### Start keeping track of variables and initialize your table\n",
        "act = [] # Accuracy\n",
        "lr = [] # Learning Rate\n",
        "epc = [] # Epoch\n",
        "hn = [] # Number of Hidden Nodes\n",
        "trcst = [] # Train Cost\n",
        "tstcst = [] # Test Cost\n",
        "tot = [] # Time to train"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJqfe4l0uhau"
      },
      "source": [
        "###**Run a baseline test with the default values for all `hyperparameters`.**\n",
        "\n",
        "#### Q1. For this training round, set `activation_type = 'sigmoid'`, `learning_rate = 0.1`, `epochs = 500`, and `hidden_nodes = 3`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CM_ipQuvuoQu"
      },
      "outputs": [],
      "source": [
        "# Define your parameters\n",
        "\n",
        "\n",
        "# Define your weights\n",
        "\n",
        "\n",
        "# Train your network (remember to calculate the time it takes to train - refer to Lab 12)\n",
        "\n",
        "\n",
        "# Calculate MSE (remember what your cost function is currently calculating in your train loop)\n",
        "\n",
        "\n",
        "# Append parameters, costs, and time to your lists\n",
        "\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "plt.plot(np.arange(epochs), train_cost, label = 'Train Cost', color = 'dodgerblue')\n",
        "plt.plot(np.arange(epochs), test_cost, label = 'Test Cost', color = 'firebrick')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(f'MSE For Train vs Test Data ({epochs} Epochs)')\n",
        "plt.ylim([0, np.max([np.max(test_cost), np.max(train_cost)])])\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QwvsFZSGzSae"
      },
      "source": [
        "### **Comparing Models with Different Activation Functions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZV1EqJozWUr"
      },
      "source": [
        "#### Q2. Train your model using the 3 different activation functions (`sigmoid`, `relu`, `tanh`) that you have created. For each activation function, record and report the training cost, testing cost, and runtime. **Make sure to append your new values to your lists for your table in the right spots!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3rv7SRcEFrXo"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(10, 4))\n",
        "\n",
        "# Define a list of what your activaiton functions are\n",
        "activation_types =\n",
        "\n",
        "# Define a dictionary of the activation functions and their colors\n",
        "activation_colors = {'sigmoid': 'red', 'relu': 'blue', 'tanh': 'green'}\n",
        "\n",
        "# Define the parameters: learning_rate and epochs (hidden nodes remain the same as your baseline)\n",
        "\n",
        "\n",
        "# Loop over the activation function types\n",
        "for activation_type in activation_types:\n",
        "  # Define your weights\n",
        "\n",
        "\n",
        "  # Train your network (remember to calculate the time it takes to train - refer to Lab 12)\n",
        "\n",
        "\n",
        "  # Calculate MSE (remember what your cost function is currently calculating in your train loop)\n",
        "\n",
        "\n",
        "  #### DO NOT ADJUST ####\n",
        "  # Add the test error line for the activation functions\n",
        "  ax[0].plot(np.arange(epochs), train_cost, label = activation_type, color = activation_colors[activation_type])\n",
        "  ax[1].plot(np.arange(epochs), test_cost, label = activation_type, color = activation_colors[activation_type])\n",
        "\n",
        "#### DO NOT ADJUST ####\n",
        "ax[0].set_title(r'Training Cost w/ Different Activation Functions')\n",
        "ax[1].set_title(r'Testing Cost w/ Different Activation Functions')\n",
        "ax[0].set_xlabel('Epochs')\n",
        "ax[1].set_xlabel('Epochs')\n",
        "ax[0].set_ylabel('Training Cost')\n",
        "ax[1].set_ylabel('Testing Cost')\n",
        "plt.legend(title = r'Activation Function', bbox_to_anchor=(1.33, 0.65))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAC4qrcjFSco"
      },
      "source": [
        "#### Q3. Activation Function Experiment Summary\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9X9X6VcFFAkt"
      },
      "source": [
        "### **Comparing Models with Different Learning Rates**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5-3-X90Gyqa"
      },
      "source": [
        "#### Q4. Compare your model with different learning rates. The learning rates you test should be `0.5, 0.1, 0.01, 0.001`. For each learning rate, calculate and report the training cost, testing cost, and runtime. **Note:** This should look very similar to what you did in the previous question for activation functions. **Make sure to append your new values to your lists for your table in the right spots!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kaSdtyZjG3kl"
      },
      "outputs": [],
      "source": [
        "# Define a list of what your learning rates are\n",
        "learning_rates =\n",
        "\n",
        "# Define a dictionary of the learning rates and their colors\n",
        "learning_rate_colors = {0.5: 'purple', 0.1: 'red', 0.01: 'blue', 0.001: 'green'}\n",
        "\n",
        "# Define the parameters: activation type and epochs (hidden nodes remain the same as your baseline)\n",
        "\n",
        "\n",
        "# Loop over the activation function types, generating weights, traning the model, tracking time, and recieving the training and testing costs (same process as above)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #### DO NOT ADJUST ####\n",
        "  plt.plot(np.arange(epochs), train_cost, label = learning_rate, color = learning_rate_colors[learning_rate])\n",
        "\n",
        "plt.title(r'Training Cost w/ Different Learning Rates ($\\eta$)')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Cost')\n",
        "plt.legend(title = r'Learning Rate ($\\eta$)', bbox_to_anchor=(1.29, 0.65))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wP8GjQvOFwkY"
      },
      "source": [
        "#### Q5. Learning Rate Experiment Summary\n",
        "\n",
        "**Additional question to answer:** From your plot, what can you say about the convergence rate for the different learning rate values? Does a larger or smaller learning rate result in a faster convergence rate?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vlZNyKnNwKxw"
      },
      "source": [
        "### **Comparing a Different Number of Hidden nodes**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h6TN3HZU1pI2"
      },
      "source": [
        "#### Q6. Compare your model with different numbers of hidden nodes. The number of hidden nodes you test should be `3, 5, 7, 9`. For each number of hidden nodes, calculate and report the training cost, testing cost, and runtime. **Note:** This should look very similar to what you did in the previous question and for activation functions. **Make sure to append your new values to your lists for your table in the right spots!**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sqBWYf8YwJ6s"
      },
      "outputs": [],
      "source": [
        "# Define a list of what your learning rates are\n",
        "num_hidden_nodes =\n",
        "\n",
        "# Define a dictionary of the learning rates and their colors\n",
        "hidden_node_colors = {3: 'purple', 5: 'red', 7: 'blue', 9: 'green'}\n",
        "\n",
        "# Define the parameters: learning_rate, epochs, and activation type\n",
        "\n",
        "\n",
        "# Loop over the activation function types, generating weights, traning the model, tracking time, and recieving the training and testing costs (same process as above)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "  #### DO NOT ADJUST ####\n",
        "  plt.plot(np.arange(epochs), train_cost, label = hidden_nodes, color = hidden_node_colors[hidden_nodes])\n",
        "\n",
        "plt.title(r'Training Cost w/ Different # Hidden Nodes')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Training Cost')\n",
        "plt.legend(title = r'# Hidden Nodes', bbox_to_anchor=(1.29, 0.65))\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EGCu44XaFW3F"
      },
      "source": [
        "#### Q6. Hidden Nodes Experiment Summary\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bMMrMqzYymh"
      },
      "source": [
        "#### **Q7: Print out your Final Table using the columns you generated in each of your experiments above**\n",
        "\n",
        "Reminder: Your table will look like the table below but with 12 rows instead of 1:\n",
        "\n",
        "| Activation Function | Learning Rate | Epochs | Hidden Nodes | Train Cost | Test Cost| Time to Train |\n",
        "| --------- |  --------- |  --------- |  --------- |  --------- |  ---------|  --------- |\n",
        "| Sigmoid | 0.1  | 500 | 3 | ?  | ? | ? |"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XzxOoFCsYzIX"
      },
      "outputs": [],
      "source": [
        "### Print final table here (shold be 12 rows total) - Make sure everything looks right!\n",
        "\n",
        "# This code will put your columns together and create a list from them.\n",
        "combined_lists = list(zip(act, lr, epc, hn, trcst, tstcst, tot))\n",
        "\n",
        "# Place your combined_lists into a pandas data frame, add column headers using the\n",
        "\n",
        "# Print your data frame out for us to view\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D9OCmgsNq7uv"
      },
      "source": [
        "**Q8: From your table shown above, what values or range of values for each parameter would you like to test further in the section below? Justify why you want to explore them more. Note: For the next section, you can try to combine different permutations of each parameter value instead of always keeping constant.**\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fr2Lk32emhGs"
      },
      "source": [
        "## Part 4: Further Experimentation/Parameter Tuning\n",
        "In this part you will experiment further to find the values of the parameters (activation function, epochs, learning rate, hidden nodes) that best train your model to produce the highest test accuracy ON OUR SYNTHETIC DATASET. This means that you need to find the parameters that not only produce the best test accuracy but also don't overfit the training data.\n",
        "\n",
        "- Do not change the cost functions, initialization method, random seed (42), or number of layers in the model (use the functions for your 1-layer model).\n",
        "\n",
        "- Include a plot for your best model of train and test error with a description of the plot including how many epochs it took to converge. Document your process by writing it out."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dyVxoMUsmi0k"
      },
      "outputs": [],
      "source": [
        "# Insert your experiments of different values below, be creative with what you try and justify your choices!\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z52O0vnwmjqN"
      },
      "outputs": [],
      "source": [
        "####### Insert Best Parameters Here (hard code these numbers) for your best 1-layer model #######\n",
        "# Leave this code chunk at the bottom of your Part 4\n",
        "epochs =\n",
        "learning_rate =\n",
        "activation_type =\n",
        "input_nodes =\n",
        "hidden_nodes =\n",
        "output_nodes =\n",
        "seed = 42"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PAL3NCc_nvId"
      },
      "source": [
        "## Part 5: Comprehension Questions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BRdmzj2rnyNC"
      },
      "source": [
        "#### Q1: What is the main difference between linear regression models and neural networks? Which piece(s) of the model introduce non-linearity into neural networks?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nZLmh7PlobAS"
      },
      "source": [
        "#### Q2: Did you notice a trade-off between training time and accuracy? Where or at what point?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vJC0plgjnyH3"
      },
      "source": [
        "#### Q3: When training multiple times, do the weights change? Is there a unique solution? Remember how OLS has a unique solution that we get to every time. To prove your answer, train 2 networks with different initial weights (i.e. using different seeds for `GenerateAllWeights()`) but the same hyperpamaters. Do the weights match after the same amount of epochs? Note: Use the training seeds provided and be sure to print the weights from Network 1 before training Network 2, otherwise they will be overwritten and you won't be able to compare.\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K5bCfhtMn_NH"
      },
      "outputs": [],
      "source": [
        "#### DO NOT ADJUST ####\n",
        "### Train your first network, print out the weights at the end of training\n",
        "training_1_seed = 370"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TpbyRInNsx-v"
      },
      "outputs": [],
      "source": [
        "#### DO NOT ADJUST ####\n",
        "### Train your second network, print out the weights at the end of training\n",
        "training_2_seed = 385"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IiuhvPPjsff6"
      },
      "source": [
        "## For TA Grading Purposes (Leave in and do not touch)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7oV_hN9MU9-3"
      },
      "outputs": [],
      "source": [
        "####### FOR TA's TO GRADE/TEST FINAL MODEL- Leave at Bottom #######\n",
        "#### DO NOT ADJUST ####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7fQ9_rGRRUT8"
      },
      "outputs": [],
      "source": [
        "# Download Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "synth_x = '/content/drive/My Drive/synth_x.csv'\n",
        "synth_x = pd.read_csv(synth_x)\n",
        "\n",
        "synth_y = '/content/drive/My Drive/synth_y.csv'\n",
        "synth_y = pd.read_csv(synth_y)\n",
        "\n",
        "synth_x = synth_x.to_numpy()\n",
        "synth_y = synth_y.to_numpy()\n",
        "\n",
        "\n",
        "# Test Student Model - Parameters defined above by student\n",
        "epochs = epochs\n",
        "learning_rate = learning_rate\n",
        "activation_type = activation_type\n",
        "input_nodes = input_nodes\n",
        "hidden_nodes = hidden_nodes\n",
        "output_nodes = output_nodes\n",
        "seed = 42\n",
        "\n",
        "# Initialization\n",
        "\n",
        "W1, W2, b1, b2 = GenerateAllWeights(input_nodes, hidden_nodes, output_nodes, seed)\n",
        "print(W1) #Initial weights\n",
        "\n",
        "# Model\n",
        "\n",
        "W1, W2, b1, b2 = GenerateAllWeights(input_nodes, hidden_nodes, output_nodes, seed)\n",
        "train_cost, test_cost = train(X_train, Y_train, X_test, Y_test, learning_rate, epochs, activation_type)\n",
        "\n",
        "train_cost = np.array(train_cost) / X_train.shape[0]\n",
        "test_cost = np.array(test_cost)\n",
        "\n",
        "print(train_cost[-1])\n",
        "print(test_cost[-1])\n",
        "print(W1) # End weights\n",
        "\n",
        "\n",
        "# Plot\n",
        "plt.plot(np.arange(epochs), train_cost, label = 'Train Cost', color = 'dodgerblue')\n",
        "plt.plot(np.arange(epochs), test_cost, label = 'Test Cost', color = 'firebrick')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Cost')\n",
        "plt.title(f'MSE For Train vs Test Data ({epochs} Epochs)')\n",
        "plt.ylim([0, np.max([np.max(test_cost), np.max(train_cost)])])\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Results\n",
        "\n",
        "their_test_accuracy = test(X_test, Y_test, activation_type)\n",
        "print(their_test_accuracy)\n",
        "\n",
        "our_test_accuracy = test(synth_x, synth_y, activation_type)\n",
        "print(our_test_accuracy)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
